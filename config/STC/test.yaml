### Experiment configuration

## General settings
split: 'test' # data split; choices ['train', 'test']
debug: false # if true, load only a few data samples
seed: 6666
validation: false # use validation; only for UBnormal
use_hr: false # for validation and test on UBnormal

## Computational resources
accelerator: 'gpu'
devices: [0] # indices of cuda devices to use

## Paths
dir_name: 'train_experiment' # name of the directory of the current experiment
data_dir: './data/HR-ShanghaiTech/' # path to the data
exp_dir: './checkpoints' # path to the directory that will contain the current experiment directory
test_path: './data/HR-ShanghaiTech/testing/test_frame_mask' # path to the test data
load_ckpt: 'checkpoint.ckpt' # name of the checkpoint to load at inference time 
create_experiment_dir: true

## WANDB configuration
use_wandb: false
project_name: "project_name"
wandb_entity: "entity_name"
group_name: "group_name"
use_ema: false

##############################


### Model's configuration

## U-Net's configuration
embedding_dim: 128 # dimension of the embedding of the UNet
dropout: 0.4 # probability of dropout
conditioning_strategy: 'no_condition' # choices ['inject' (add2layers), 'concat' (cat), 'inbetween_imp' (interleave), 'random_imp' (random_indices), 'no_condition' (none)]

## Conditioning network's configuration
conditioning_architecture: 'AE' # choices ['AE', 'E', 'E_unet']
conditioning_indices: [] # If conditioning_strategy=random_imp, it must be int and it is used as the number of random indices that will be selected
                              # If an int is given and conditioning_strategy=[inject|concat], n_frames//conditioning_indices will be used as the number of conditioning indices
                              # If a list is given, it will be used as the conditioning indices 
h_dim: 32 # dimension of the bottleneck at the end of the encoder of the conditioning network
latent_dim: 128 # dimension of the latent space of the conditioning encoder
channels: [32,16,32] # channels for the encoder
unet_down_channels:  [16, 32, 64, 128, 128, 128, 128] #[16, 32, 32, 64, 64, 128, 64], [16, 32, 64, 128, 128, 128, 128], [16, 32, 128, 256, 256, 512, 256]
unet_up_channels:  [128, 64, 32, 2] #[64, 32, 32, 2], [128, 64, 32, 2], [256, 128, 32, 2]
##############################


### Training's configuration

## Diffusion's configuration
noise_steps: 20 # how many diffusion steps to perform

### Optimizer and scheduler's configuration
n_epochs: 100
n_epochs_tune: 100
opt_lr: 0.01
opt: 'Adam' #[Adam, SGD, AdamW, NAdam, Adamax, RAdam,RMSprop,Adagrad, Adadelta]

## Losses' configuration
loss_fn: 'mse' # loss function; choices ['mse', 'l1', 'smooth_l1']
#rec_weight: 1. # weight of the reconstruction loss

##############################


### Inference's configuration
n_generated_samples: 1 # number of samples to generate
model_return_value: 'loss' # choices ['loss', 'poses', 'all']; if 'loss', the model will return the loss; 
                           # if 'poses', the model will return the generated poses; 
                           # if 'all', the model will return both the loss and the generated poses
aggregation_strategy: 'best' # choices ['best', 'mean', 'median', 'random']; if 'best', the best sample will be selected; 
                             # if 'mean', the mean of loss of the samples will be selected; 
                             # if 'median', the median of the loss of the samples will be selected; 
                             # if 'random', a random sample will be selected;
                             # if 'mean_poses', the mean of the generated poses will be selected;
                             # if 'median_poses', the median of the generated poses will be selected;
                             # if 'all', all the generated poses will be selected
filter_kernel_size: 18 # size of the kernel to use for smoothing the anomaly score of each clip
frames_shift: 8 # it compensates the shift of the anomaly score due to the sliding window; 
                # in conjuction with pad_size and filter_kernel_size, it strongly depends on the dataset
save_tensors: true # if true, save the generated tensors for faster inference
load_tensors: false # if true, load the generated tensors for faster inference

##############################


### Dataset's configuration

## Important parameters
dataset_choice: 'HR-STC'
seg_len: 12 # length of the window (cond+noised) 
vid_res: [856,480]
batch_size: 512
pad_size: -1 # size of the padding 

## Other parameters
headless: false # remove the keypoints of the head
hip_center: false # center the keypoints on the hip
kp18_format: false # use the 18 keypoints format
normalization_strategy: 'robust' # use 'none' to avoid normalization, 'robust' otherwise
num_coords: 2 # number of coordinates to use
num_transform: 1 # number of transformations to apply
num_workers: 16
seg_stride: 6 # stride of the sliding window
seg_th: 0
start_offset: 0
symm_range: true
use_fitted_scaler: false
perturb: true
weight_perturb: 0.1
masked_rate: 0.5
masked_rate_dct: 0.1
dct: true